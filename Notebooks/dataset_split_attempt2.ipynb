{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c17fa8b",
   "metadata": {},
   "source": [
    "# Splitting update1_fulltrain.pkl into training, test, and validation sets\n",
    "Challenge here was to ensure splitting was not done on the recording level, and also manually assign audio to the val dataset ensuring that recordings of the same bird did not end up in the val dataset.\\\n",
    "Also had to slim the train and val df to just one \"virail\" column and not a column for each XC species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cnn module provides classes for training/predicting with various types of CNNs\n",
    "from opensoundscape import CNN\n",
    "\n",
    "#other utilities and packages\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import subprocess\n",
    "from glob import glob\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#set up plotting\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for large visuals\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ff987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"/home/brg226/projects/vira_beg/training_data/update2_fulltrain_with_field.pkl\") \n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb377a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names\n",
    "print(\"Column names:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nNumber of columns: {len(df.columns)}\")\n",
    "print(f\"Index name: {df.index.name}\")\n",
    "print(f\"DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the DataFrame structure more deeply\n",
    "print(\"=== DETAILED DATAFRAME INVESTIGATION ===\")\n",
    "print(f\"DataFrame type: {type(df)}\")\n",
    "print(f\"Index type: {type(df.index)}\")\n",
    "print(f\"Columns type: {type(df.columns)}\")\n",
    "\n",
    "print(f\"\\n=== INDEX INFORMATION ===\")\n",
    "print(f\"Index names: {df.index.names}\")\n",
    "print(f\"Index nlevels: {df.index.nlevels}\")\n",
    "if hasattr(df.index, 'levels'):\n",
    "    print(f\"Index levels: {len(df.index.levels)} levels\")\n",
    "    for i, level in enumerate(df.index.levels):\n",
    "        print(f\"  Level {i}: {level[:5].tolist()}...\" if len(level) > 5 else f\"  Level {i}: {level.tolist()}\")\n",
    "\n",
    "print(f\"\\n=== COLUMNS INFORMATION ===\")  \n",
    "print(f\"Column names: {df.columns.names}\")\n",
    "print(f\"Column nlevels: {df.columns.nlevels}\")\n",
    "print(f\"Actual columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\n=== DATAFRAME STRUCTURE ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nDataFrame info:\")\n",
    "df.info()\n",
    "\n",
    "print(f\"\\n=== SAMPLE DATA ===\")\n",
    "print(\"First few rows with reset_index():\")\n",
    "try:\n",
    "    print(df.reset_index().head())\n",
    "except:\n",
    "    print(\"reset_index() failed\")\n",
    "    print(\"Regular head():\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have the file column\n",
    "if 'file' in df.columns:\n",
    "    print(f\"Success! 'file' column found with {df['file'].nunique()} unique files\")\n",
    "    print(f\"Sample file values: {df['file'].head(3).tolist()}\")\n",
    "else:\n",
    "    print(\"'file' column not found in columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef2598",
   "metadata": {},
   "source": [
    "From Sam \\\n",
    "Workflow might look like:\n",
    "select a test set if you have a good one: eg, representative of the field data;\\\n",
    "combine all remaining positives and negatives into one big tran&val set\\\n",
    "split into training and validation with file-level splitting\\\n",
    "resample training set for even class representation\\\n",
    "Does that make sense?\\\n",
    "In your case there is no good test set, you’ll just have a train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual file splitting - specify validation files explicitly + 20% of NON-virail files\n",
    "audio_path = \"/home/brg226/projects/vira_beg/training_data/annotated_positive_audio/audio_viratrain\"\n",
    "manual_val_files = [f\"{audio_path}/104566671.wav\", f\"{audio_path}/252407901.wav\", f\"{audio_path}/357857961.wav\"]\n",
    "\n",
    "# Get all unique files\n",
    "unique_files = df['file'].unique()\n",
    "\n",
    "# Check which manual files exist\n",
    "manual_files_found = [f for f in manual_val_files if f in unique_files]\n",
    "print(f\"Manual validation files found: {len(manual_files_found)}\")\n",
    "for file in manual_files_found:\n",
    "    print(f\"✓ {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get files that come from virail column (positive examples)\n",
    "virail_files = df[df['virail'] == 1]['file'].unique()\n",
    "print(f\"\\nTotal files from virail column: {len(virail_files)}\")\n",
    "\n",
    "# Remaining virail files (not in manual validation) go to training\n",
    "remaining_virail_files = [f for f in virail_files if f not in manual_files_found]\n",
    "print(f\"Remaining virail files (going to training): {len(remaining_virail_files)}\")\n",
    "\n",
    "# Get non-virail files (negative examples)\n",
    "non_virail_files = df[df['virail'] != 1]['file'].unique()\n",
    "print(f\"Non-virail files available: {len(non_virail_files)}\")\n",
    "\n",
    "# Split non-virail files 80/20\n",
    "if non_virail_files.size > 0:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_files_non_virail, val_files_non_virail = train_test_split(\n",
    "        non_virail_files, test_size=0.2, random_state=42\n",
    "    )\n",
    "else:\n",
    "    train_files_non_virail, val_files_non_virail = [], []\n",
    "\n",
    "# Combine for final sets\n",
    "val_files = list(manual_files_found) + list(val_files_non_virail)\n",
    "train_files = list(remaining_virail_files) + list(train_files_non_virail)\n",
    "\n",
    "print(f\"\\nFinal split result:\")\n",
    "print(f\"Validation files: {len(val_files)} files ({len(manual_files_found)} manual virail + {len(val_files_non_virail)} non-virail)\")\n",
    "print(f\"Training files: {len(train_files)} files ({len(remaining_virail_files)} remaining virail + {len(train_files_non_virail)} non-virail)\")\n",
    "print(f\"Total files: {len(val_files) + len(train_files)}\")\n",
    "\n",
    "print(f\"\\nValidation files breakdown:\")\n",
    "print(f\"  Manual virail files: {len(manual_files_found)}\")\n",
    "print(f\"  Auto-selected non-virail files: {len(val_files_non_virail)}\")\n",
    "print(f\"\\nTraining files breakdown:\")\n",
    "print(f\"  Remaining virail files: {len(remaining_virail_files)}\")\n",
    "print(f\"  Auto-selected non-virail files: {len(train_files_non_virail)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fabb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate training and validation DataFrames based on file splits\n",
    "train_df = df[df['file'].isin(train_files)]\n",
    "val_df = df[df['file'].isin(val_files)]\n",
    "\n",
    "print(f\"Training set: {len(train_df)} clips from {len(train_files)} files\")\n",
    "print(f\"Validation set: {len(val_df)} clips from {len(val_files)} files\")\n",
    "print(f\"Total clips: {len(train_df) + len(val_df)}\")\n",
    "\n",
    "# Check class distribution in each set\n",
    "if 'virail' in df.columns:\n",
    "    print(f\"\\nTraining set class distribution:\")\n",
    "    print(train_df['virail'].value_counts())\n",
    "    print(f\"\\nValidation set class distribution:\")\n",
    "    print(val_df['virail'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ddd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slimmed training set - randomly select 5,000 non-virail rows\n",
    "print(\"Creating slimmed training set...\")\n",
    "\n",
    "# Separate virail and non-virail rows in training set\n",
    "train_virail = train_df[train_df['virail'] == 1]\n",
    "train_non_virail = train_df[train_df['virail'] != 1]\n",
    "\n",
    "print(f\"Original training set:\")\n",
    "print(f\"  Virail rows: {len(train_virail)}\")\n",
    "print(f\"  Non-virail rows: {len(train_non_virail)}\")\n",
    "\n",
    "# Randomly sample 5,000 non-virail rows\n",
    "if len(train_non_virail) > 5000:\n",
    "    train_non_virail_slim = train_non_virail.sample(n=5000, random_state=42)\n",
    "    print(f\"\\nSlimmed non-virail rows: {len(train_non_virail_slim)} (sampled from {len(train_non_virail)})\")\n",
    "else:\n",
    "    train_non_virail_slim = train_non_virail\n",
    "    print(f\"\\nUsing all {len(train_non_virail_slim)} non-virail rows (less than 5,000 available)\")\n",
    "\n",
    "# Combine virail rows with slimmed non-virail rows\n",
    "train_df_slim = pd.concat([train_virail, train_non_virail_slim], ignore_index=True)\n",
    "\n",
    "print(f\"\\nSlimmed training set:\")\n",
    "print(f\"  Total rows: {len(train_df_slim)}\")\n",
    "print(f\"  Virail rows: {len(train_df_slim[train_df_slim['virail'] == 1])}\")\n",
    "print(f\"  Non-virail rows: {len(train_df_slim[train_df_slim['virail'] != 1])}\")\n",
    "\n",
    "print(f\"\\nClass distribution in slimmed training set:\")\n",
    "print(train_df_slim['virail'].value_counts())\n",
    "\n",
    "# Report how many 1s from each column are in the slim training set\n",
    "print(f\"\\nDetailed breakdown of 1s and 1.0s in slimmed training set:\")\n",
    "for col in train_df_slim.columns:\n",
    "    try:\n",
    "        ones_count = ((train_df_slim[col] == 1) | (train_df_slim[col] == 1.0)).sum()\n",
    "        print(f\"  {col}: {ones_count} rows with value 1 or 1.0\")\n",
    "    except:\n",
    "        print(f\"  {col}: Cannot check for 1s/1.0s (non-numeric column)\")\n",
    "\n",
    "# Update train_df to use the slimmed version\n",
    "train_df = train_df_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of all columns, especially target columns\n",
    "print(\"=== DATA TYPE ANALYSIS ===\")\n",
    "print(f\"Column data types in training set:\")\n",
    "for i, (col, dtype) in enumerate(train_df.dtypes.items()):\n",
    "    print(f\"  {i}: {col} -> {dtype}\")\n",
    "\n",
    "# Focus on columns 3-34 (likely your target/label columns)\n",
    "print(f\"\\n=== TARGET COLUMNS (3-34) ANALYSIS ===\")\n",
    "target_cols = train_df.columns[3:35]  # columns 3-34 (0-indexed, so 3:35)\n",
    "print(f\"Target columns: {target_cols.tolist()}\")\n",
    "\n",
    "# Check if all target columns have the same data type\n",
    "target_dtypes = [train_df[col].dtype for col in target_cols]\n",
    "unique_dtypes = set(target_dtypes)\n",
    "print(f\"\\nUnique data types in target columns: {unique_dtypes}\")\n",
    "\n",
    "if len(unique_dtypes) == 1:\n",
    "    print(f\"✓ All target columns have the same data type: {list(unique_dtypes)[0]}\")\n",
    "else:\n",
    "    print(\"⚠ Target columns have different data types:\")\n",
    "    for col in target_cols:\n",
    "        print(f\"  {col}: {train_df[col].dtype}\")\n",
    "\n",
    "# Check unique values in a few target columns to understand the data\n",
    "print(f\"\\n=== SAMPLE TARGET COLUMN VALUES ===\")\n",
    "for col in target_cols[:5]:  # Check first 5 target columns\n",
    "    unique_vals = train_df[col].unique()\n",
    "    print(f\"{col}: {unique_vals[:10]}...\")  # Show first 10 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d047e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target columns (3-34) to int64\n",
    "print(\"=== CONVERTING TARGET COLUMNS TO INT64 ===\")\n",
    "target_cols = train_df.columns[3:35]  # columns 3-34\n",
    "\n",
    "print(\"Converting target columns to int64...\")\n",
    "for col in target_cols:\n",
    "    old_dtype = train_df[col].dtype\n",
    "    train_df[col] = train_df[col].astype('int64')\n",
    "    print(f\"  {col}: {old_dtype} -> {train_df[col].dtype}\")\n",
    "\n",
    "# Also convert validation set to match - fix the copy warning\n",
    "print(f\"\\nConverting validation set target columns to int64...\")\n",
    "val_df = val_df.copy()  # Create explicit copy to avoid SettingWithCopyWarning\n",
    "for col in target_cols:\n",
    "    if col in val_df.columns:\n",
    "        old_dtype = val_df[col].dtype\n",
    "        val_df[col] = val_df[col].astype('int64')\n",
    "        print(f\"  {col}: {old_dtype} -> {val_df[col].dtype}\")\n",
    "\n",
    "# Verify all target columns are now int64\n",
    "print(f\"\\n=== VERIFICATION ===\")\n",
    "target_dtypes = [train_df[col].dtype for col in target_cols]\n",
    "unique_dtypes = set(target_dtypes)\n",
    "print(f\"Unique data types in target columns after conversion: {unique_dtypes}\")\n",
    "\n",
    "if len(unique_dtypes) == 1 and 'int64' in str(list(unique_dtypes)[0]):\n",
    "    print(\"✓ All target columns successfully converted to int64\")\n",
    "else:\n",
    "    print(\"⚠ Some columns may not have converted properly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the head of the DataFrame with updated data types\n",
    "print(\"=== TRAINING DATAFRAME WITH UPDATED DATA TYPES ===\")\n",
    "print(f\"Training DataFrame shape: {train_df.shape}\")\n",
    "print(f\"Training DataFrame dtypes:\")\n",
    "print(train_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slimmed validation set - randomly select 1,000 non-virail rows\n",
    "print(\"=== SLIMMING VALIDATION SET ===\")\n",
    "val_virail = val_df[val_df['virail'] == 1]\n",
    "val_non_virail = val_df[val_df['virail'] != 1]\n",
    "\n",
    "print(f\"Original validation set:\")\n",
    "print(f\"  Virail rows: {len(val_virail)}\")\n",
    "print(f\"  Non-virail rows: {len(val_non_virail)}\")\n",
    "\n",
    "# Randomly sample 1,000 non-virail rows\n",
    "if len(val_non_virail) > 1000:\n",
    "    val_non_virail_slim = val_non_virail.sample(n=1000, random_state=42)\n",
    "    print(f\"\\nSlimmed validation non-virail rows: {len(val_non_virail_slim)} (sampled from {len(val_non_virail)})\")\n",
    "else:\n",
    "    val_non_virail_slim = val_non_virail\n",
    "    print(f\"\\nUsing all {len(val_non_virail_slim)} non-virail rows (less than 1,000 available)\")\n",
    "\n",
    "# Combine virail rows with slimmed non-virail rows for validation\n",
    "val_df_slim = pd.concat([val_virail, val_non_virail_slim], ignore_index=True)\n",
    "\n",
    "print(f\"\\nSlimmed validation set:\")\n",
    "print(f\"  Total rows: {len(val_df_slim)}\")\n",
    "print(f\"  Virail rows: {len(val_df_slim[val_df_slim['virail'] == 1])}\")\n",
    "print(f\"  Non-virail rows: {len(val_df_slim[val_df_slim['virail'] != 1])}\")\n",
    "\n",
    "print(f\"\\nClass distribution in slimmed validation set:\")\n",
    "print(val_df_slim['virail'].value_counts())\n",
    "\n",
    "# Update val_df to use the slimmed version\n",
    "val_df = val_df_slim\n",
    "\n",
    "# Create new DataFrame with only essential columns\n",
    "essential_cols = ['file', 'start_time', 'end_time', 'virail']\n",
    "train_df_essential = train_df[essential_cols].copy()\n",
    "val_df_essential = val_df[essential_cols].copy()\n",
    "\n",
    "# Fix data types for validation set to match training set\n",
    "print(\"\\n=== FIXING VALIDATION SET DATA TYPES ===\")\n",
    "print(\"Validation set data types before fix:\")\n",
    "print(val_df_essential.dtypes)\n",
    "\n",
    "# Convert virail to int64 in validation set if needed\n",
    "if val_df_essential['virail'].dtype != 'int64':\n",
    "    val_df_essential['virail'] = val_df_essential['virail'].astype('int64')\n",
    "    print(\"✓ Converted virail column to int64\")\n",
    "\n",
    "print(\"\\nValidation set data types after fix:\")\n",
    "print(val_df_essential.dtypes)\n",
    "\n",
    "print(\"\\n=== ESSENTIAL COLUMNS DATAFRAMES ===\")\n",
    "print(f\"Training set essential columns: {essential_cols}\")\n",
    "print(f\"Training DataFrame shape: {train_df_essential.shape}\")\n",
    "print(f\"Training DataFrame dtypes:\")\n",
    "print(train_df_essential.dtypes)\n",
    "\n",
    "print(f\"\\nTraining DataFrame head:\")\n",
    "print(train_df_essential.head())\n",
    "\n",
    "print(f\"\\nValidation DataFrame head:\")\n",
    "print(val_df_essential.head())\n",
    "\n",
    "print(f\"\\nClass distribution in essential training set:\")\n",
    "print(train_df_essential['virail'].value_counts())\n",
    "\n",
    "print(f\"\\nClass distribution in essential validation set:\")\n",
    "print(val_df_essential['virail'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ddde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both full and essential DataFrames as pickle files with date\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"/home/brg226/projects/vira_beg/training_data/second_pass_nov25\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get current date for filename\n",
    "date_str = \"nov25_2025\"\n",
    "\n",
    "print(\"=== SAVING FULL DATAFRAMES ===\")\n",
    "# Save full training DataFrame (all columns)\n",
    "train_full_output_path = os.path.join(output_dir, f\"train_df_full_{date_str}.pkl\")\n",
    "train_df.to_pickle(train_full_output_path)\n",
    "print(f\"Saved full training DataFrame to: {train_full_output_path}\")\n",
    "print(f\"Full training set shape: {train_df.shape}\")\n",
    "print(f\"Full training set columns: {len(train_df.columns)}\")\n",
    "\n",
    "# Save full validation DataFrame (all columns)\n",
    "val_full_output_path = os.path.join(output_dir, f\"val_df_full_{date_str}.pkl\")\n",
    "val_df.to_pickle(val_full_output_path)\n",
    "print(f\"Saved full validation DataFrame to: {val_full_output_path}\")\n",
    "print(f\"Full validation set shape: {val_df.shape}\")\n",
    "print(f\"Full validation set columns: {len(val_df.columns)}\")\n",
    "\n",
    "print(\"\\n=== SAVING ESSENTIAL DATAFRAMES ===\")\n",
    "# Save essential training DataFrame (4 columns only)\n",
    "train_output_path = os.path.join(output_dir, f\"train_df_essential_{date_str}.pkl\")\n",
    "train_df_essential.to_pickle(train_output_path)\n",
    "print(f\"Saved essential training DataFrame to: {train_output_path}\")\n",
    "print(f\"Essential training set shape: {train_df_essential.shape}\")\n",
    "\n",
    "# Save essential validation DataFrame (4 columns only)\n",
    "val_output_path = os.path.join(output_dir, f\"val_df_essential_{date_str}.pkl\")\n",
    "val_df_essential.to_pickle(val_output_path)\n",
    "print(f\"Saved essential validation DataFrame to: {val_output_path}\")\n",
    "print(f\"Essential validation set shape: {val_df_essential.shape}\")\n",
    "\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"All DataFrames saved to: {output_dir}\")\n",
    "print(f\"\\nFull datasets:\")\n",
    "print(f\"  - train_df_full_{date_str}.pkl\")\n",
    "print(f\"  - val_df_full_{date_str}.pkl\")\n",
    "print(f\"\\nEssential datasets:\")\n",
    "print(f\"  - train_df_essential_{date_str}.pkl\")\n",
    "print(f\"  - val_df_essential_{date_str}.pkl\")\n",
    "\n",
    "print(f\"\\nFinal dataset summary:\")\n",
    "print(f\"Training set: {len(train_df_essential)} clips\")\n",
    "print(f\"  - Virail=1: {(train_df_essential['virail'] == 1).sum()}\")\n",
    "print(f\"  - Virail=0: {(train_df_essential['virail'] == 0).sum()}\")\n",
    "\n",
    "print(f\"Validation set: {len(val_df_essential)} clips\")\n",
    "print(f\"  - Virail=1: {(val_df_essential['virail'] == 1).sum()}\")\n",
    "print(f\"  - Virail=0: {(val_df_essential['virail'] == 0).sum()}\")\n",
    "\n",
    "print(f\"\\nEssential columns: {train_df_essential.columns.tolist()}\")\n",
    "print(f\"Essential data types: {train_df_essential.dtypes.to_dict()}\")\n",
    "print(f\"Full dataset has {len(train_df.columns)} total columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
