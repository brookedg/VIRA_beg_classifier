{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a92becc",
   "metadata": {},
   "source": [
    "# Adding random field audio to my negatives dataset\n",
    "1. Generate list of random field audio files from rloc2025a, one from each recorder. \n",
    "2. Create .pkl file with each hour split into 1.5 second chunks, matching the df format of the train anf val dfs. \n",
    "3. Integrate that field_negatives.pkl into my existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def randomly_select_wav_files(root_dir):\n",
    "    \"\"\"\n",
    "    Randomly select one .wav file from each subdirectory and create 1.5 second clips\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    selected_files = []\n",
    "    \n",
    "    # Find all subdirectories\n",
    "    subdirs = [d for d in root_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f\"Found {len(subdirs)} subdirectories in {root_dir}\")\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        # Find all .wav files in this subdirectory\n",
    "        wav_files = list(subdir.glob(\"*.WAV\"))\n",
    "        \n",
    "        if wav_files:\n",
    "            # Randomly select one file\n",
    "            selected_file = random.choice(wav_files)\n",
    "            \n",
    "            # Create 1.5 second clips from 15-minute file\n",
    "            file_duration = 15 * 60  # 15 minutes = 900 seconds\n",
    "            clip_duration = 1.5\n",
    "            \n",
    "            # Calculate number of clips\n",
    "            num_clips = int(file_duration / clip_duration)\n",
    "            \n",
    "            for clip_num in range(num_clips):\n",
    "                start_time = clip_num * clip_duration\n",
    "                end_time = start_time + clip_duration\n",
    "                \n",
    "                selected_files.append({\n",
    "                    'subdirectory': subdir.name,\n",
    "                    'selected_file': selected_file.name,\n",
    "                    'full_path': str(selected_file),\n",
    "                    'total_wav_files': len(wav_files),\n",
    "                    'start_time': start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'field_data': 1,\n",
    "                    'clip_number': clip_num + 1\n",
    "                })\n",
    "            \n",
    "            print(f\"{subdir.name}: Selected {selected_file.name} from {len(wav_files)} .WAV files, created {num_clips} clips\")\n",
    "        else:\n",
    "            print(f\"{subdir.name}: No .WAV files found\")\n",
    "    \n",
    "    return selected_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec25a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Select files\n",
    "root_directory = \"/media/kiwi/datasets/unfinalized/rloc2025a\"\n",
    "selected_files = randomly_select_wav_files(root_directory)\n",
    "\n",
    "print(f\"\\nTotal selected files: {len(selected_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to DataFrame for easier viewing and manipulation\n",
    "if selected_files:\n",
    "    df_selected = pd.DataFrame(selected_files)\n",
    "    print(\"\\nSelected files summary:\")\n",
    "    print(df_selected)\n",
    "    \n",
    "    # Save to CSV for reference\n",
    "    output_file = \"/home/brg226/projects/vira_beg/training_data/field_negatives/selected_field_negatives.csv\"\n",
    "    df_selected.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSaved selected files to {output_file}\")\n",
    "else:\n",
    "    print(\"No files were selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62467c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename and clean up the DataFrame\n",
    "df_selected = df_selected.rename(columns={'full_path': 'file'})\n",
    "df_selected = df_selected.drop(columns=['clip_number', 'subdirectory', 'total_wav_files', 'selected_file'])\n",
    "df_selected = df_selected.reset_index(drop=True)\n",
    "print(\"Cleaned DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3667168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e973d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame as a pickle file\n",
    "pkl_output_file = \"/home/brg226/projects/vira_beg/training_data/field_negatives/field_negatives.pkl\"\n",
    "df_selected.to_pickle(pkl_output_file)\n",
    "print(f\"Saved cleaned DataFrame to {pkl_output_file}\")\n",
    "\n",
    "print(f\"\\nFinal DataFrame shape: {df_selected.shape}\")\n",
    "print(f\"Columns: {df_selected.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897456cd",
   "metadata": {},
   "source": [
    "# Merge field_negatives.pkl with update1_fulltrain.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bf60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original training dataset\n",
    "update1_path = \"/home/brg226/projects/vira_beg/training_data/update1_fulltrain.pkl\"\n",
    "df_original = pd.read_pickle(update1_path)\n",
    "\n",
    "print(f\"Original dataset shape: {df_original.shape}\")\n",
    "print(f\"Original columns: {df_original.columns.tolist()}\")\n",
    "print(f\"Index type: {type(df_original.index)}\")\n",
    "print(f\"Index names: {df_original.index.names}\")\n",
    "\n",
    "# Convert MultiIndex to regular columns if needed\n",
    "if hasattr(df_original.index, 'nlevels') and df_original.index.nlevels > 1:\n",
    "    print(\"Converting MultiIndex to regular columns...\")\n",
    "    df_original = df_original.reset_index()\n",
    "    print(f\"After reset_index - shape: {df_original.shape}\")\n",
    "    print(f\"After reset_index - columns: {df_original.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"Index is not MultiIndex, no conversion needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add field_data column to original dataset (fill with 0 for existing data)\n",
    "df_original['field_data'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47661048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nField negatives dataset shape: {df_selected.shape}\")\n",
    "print(f\"Field negatives columns: {df_selected.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b820c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get columns that exist in original but not in field negatives\n",
    "original_only_cols = set(df_original.columns) - set(df_selected.columns)\n",
    "print(f\"\\nColumns in original dataset that need to be added to field negatives: {original_only_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7a3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add missing columns to field_negatives with 0 values\n",
    "for col in original_only_cols:\n",
    "    df_selected[col] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27246d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reorder field_negatives columns to match original dataset\n",
    "df_selected = df_selected[df_original.columns]\n",
    "\n",
    "print(f\"\\nAligned field negatives shape: {df_selected.shape}\")\n",
    "print(f\"Aligned field negatives columns: {df_selected.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate the datasets\n",
    "df_merged = pd.concat([df_original, df_selected], ignore_index=True)\n",
    "\n",
    "print(f\"\\nMerged dataset shape: {df_merged.shape}\")\n",
    "print(f\"Merged dataset columns: {df_merged.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check field_data distribution\n",
    "print(f\"\\nField data distribution:\")\n",
    "print(df_merged['field_data'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ceb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the merged dataset\n",
    "merged_output_path = \"/home/brg226/projects/vira_beg/training_data/update2_fulltrain_with_field.pkl\"\n",
    "df_merged.to_pickle(merged_output_path)\n",
    "print(f\"\\nSaved merged dataset to: {merged_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb757d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nFirst few rows of merged dataset:\")\n",
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nLast few rows of merged dataset (should be field data):\")\n",
    "df_merged.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
